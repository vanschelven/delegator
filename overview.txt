report voor enige verbetering (stroom ingeplugd).txt

Blah blah what do things mean?
Comparisons have been made with the most simple variants in normal Java, i.e. when creating objects we actually speak about java.lang.Object. This means expected ratios will be more favourable for Delegator since the constant overhead (proove this) will be smaller in comparison with larger objects.

I've used large number (c. 100.000 calls etc.).
1. To be able to measure anything.
2. Because this is the only interesting scenario anyway... Even without any optimalisations my old .... 500 mhz pentium III could fit 10.000 delegator method calls into a second.
This also may mean the optimalisations are not really optimalisations for really small numbers but who cares?
I havn't been careful not to use too much memory but I havn't used much either. I don't think caching will be a problem.

I used the rythm of scenario's - profile - optimize - repeat...

Most optimizations are introductions of caches. Some are in surprising spots.
The java.reflect framework turned out to be extraordinarily slow, so some optimazations are simply caching of references to objects from that framework.

Some are of a surprising shape. Most are HashMaps of some kind.
Some performance optimizations are really removals of unnecissary rubish.
Another form of caching that often resulted in another 20% was caching of some result in a local variable, or skipping contains() calls and checking for nulls.

In speeding up a large number of method calls the first and largest improvement was to introduce a cache for called methods.

Remember Self's method lookup mechanism? Whenever a method is called on Self using invoke this mechanism was used. It basically matches the method with the first matching method of one of Self's components.
Using this lookup mechanism calling methods turned out to be roughly 2000 - 4000 times more slow than doing a method call in normal Java.
Methods that were called on components that were checked later tended to have higher ratios here, because the way the mechanism works is as follows:
First the first component is checked for a matching method, then the second component etc. This naturally leads to worse performance for methods that appear only in more highly numbered components.

The first and largest performance improvement was to introduce a cache for called methods.
Each self is given a cache, i.e. some wrapper for a HashMap that stores called methods as keys and a tuple of actually executed methods and the index of the component these were called on as values.

Extra benifits were gained by "silly improvements" like exposing this tuple so that the fetching of the tuple was done only once.

These improvements brought down the ratio to called methods to c. 1:400 (introduction of a cache)

Secondly:
It turned out that the generated proxies called super.getClass()....getDeclaredMethod( <- zoek uit.
This java.reflect.Method was then passed to Self to be resolved.
In fact this was unneccasary since the method resolving mechanism only used a couple of properties of the method that were available in the first place (in fact they were used to fetch this java.reflect.Method).
This improvement brought down the ration to ?????

1:85 (exposing tuples) <- this was done even later
ik denk dat "proxy opzet cache" betekent dat hij niet direct een HashMap aanmaakt. Wellicht verklaart dat met een of andere check op null waarom die daar versnelt.



The special signature used to be constructed by havind an extra argument, Self. This was, however, no longer used since the introduction of a stack to do that. We changed the special signature to have a different name instead of a  different number of arguments.
This led to a ratio of c. 1:30.

Removing a call to contains brought this down to 1:28.

Omzetten cache naar array - what the hell is dit?? Is dit dat cache calls ontvangt naar nummers? Denk het wel! 1:23
Every time a method is called on a proxy this was forwarded to Self.invoke. This is done by sending a number of bits of information: name, modifiers, etc.
Since we use a cache 

In fact the number of methods that can be called on proxies is limited to...

report voor enige optimalisering op parameters.txt
report na optimalisering op parameters (versimpling van MyInvocationHandler).txt -- ik denk dat deze het echt beperkt tot het nummertje

report voor aanpassingen newcalls.txt
report na introductie cache over meerdere objecten.txt
Dit heeft alleen effect voor meerdere objecten

report na verwijdering zeroAllFields.txt
Dit heeft effect op het maken van nieuwe proxies en componenten.

report incl test voor casts.txt
report na correctie in cast (create van self uit de loop).txt
report na uitschakelen finalize.txt
finalize bleek een hoop tijd te vreten... en da's niet nodig.

report na cachen self-field.txt
Het self field... uitleg wat het is en dat het traag is... brengt casts naar 1:9

report na profiling composition.txt
report na correctie bug removing.txt

report na introductie cacheNodes.txt
dit brengt (geloof ik) composition van 1:3 naar 1:9

A final speedup of the changing of compositions was achieved as follows:

CacheNodes represent lists of classes and the methodsCache for those lists.
Additionally they maintain various references to other CacheNodes using some path.
A general cache maintains a map for all CacheNodes...

Because of the introduction of a special node for the empty cache and the fact that creation is usually coupled to one change of composition this also positively contributes to the speed of creation.



Nieuwe opzet: we beginnen bij het eindresultaat.

Creation of proxies
	numbers based on a MethodRegister, calls using that number

Self invoke
	MethodsCache (explain this is an array of tuples)
	known? -> use it
	unknown -> (1) find the name & other stuff in the MethodRegister, (2) resolve (3) store

Self creation:
	new emptyNode, which has a cache
	
Self change of composition
	pass the change on to a CacheNode (what is this)
	CacheNode: if known transer do it
	else ask CacheCache
		if known return
		else store
		store
	get MethodsCache from CacheNode


(workaround for interface calls - check hiervan ook performance!)